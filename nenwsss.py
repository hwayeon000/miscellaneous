# from openai import OpenAI
# client = OpenAI(
#   api_key=API_KEY
# )

# response = client.responses.create(
#   model="gpt-4o-mini",
#   input="ìµœê·¼ AI ë™í–¥ì„ 2025ë…„ ìµœì‹ ìœ¼ë¡œ ê²€ìƒ‰í•´ ì •ë¦¬í•´ ì¤„ ìˆ˜ ìˆì–´?",
#   store=True,
# )

# print(response.output_text);
# 1 end =======================================

# import os
# os.environ["OPENAI_API_KEY"] = API_KEY
#  langchain ê³¼ langchain-openaië¥¼ install í•´ì¤ë‹ˆë‹¤.
# from langchain_openai import ChatOpenAI
# from langchain_core.messages import HumanMessage
# from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

# chat = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=1, streaming=True, callbacks=[StreamingStdOutCallbackHandler()])

# response = chat.predict("why python is the most popular language? answer in Korean")

# print(response)
# 2 end ======================================ì´ê±´ ì•ˆë¨


import os
import uuid
import asyncio
from typing import List, Dict, Any, AsyncGenerator

from fastapi import FastAPI, Request
from fastapi.responses import HTMLResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates

# ì‹¤ì œ OpenAI API ì—°ë™ìš©
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage, BaseMessage
from langchain.callbacks.base import AsyncCallbackHandler

import tiktoken
from dotenv import load_dotenv

# ----- í™˜ê²½ë³€ìˆ˜ ë¡œë“œ -----
load_dotenv()
API_KEY = os.getenv('GPT_API_KEY')  # .envì— ì €ì¥ëœ API_KEY ë¶ˆëŸ¬ì˜¤ê¸°

# ----- ê¸°ë³¸ ì„¤ì • -----
MODEL_NAME = "gpt-3.5-turbo"
TEMPERATURE = 1.0
MAX_CONTEXT_TOKENS = 1200
SYSTEM_PROMPT = (
    "ë„ˆëŠ” ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µí•˜ëŠ” í•œêµ­ì–´ ë¹„ì„œì•¼. "
    "ë‹µë³€ì´ ê¸¸ì–´ì§ˆ ë•ŒëŠ” í•µì‹¬ë¶€í„° ìš”ì•½í•˜ê³ , ì½”ë“œ/ëª…ë ¹ì€ ë¸”ë¡ìœ¼ë¡œ ì œì‹œí•´ì¤˜."
)

# FastAPI ê¸°ë³¸ ì„¸íŒ…
app = FastAPI()
templates = Jinja2Templates(directory="templates")
app.mount("/static", StaticFiles(directory="static"), name="static")

# ì„¸ì…˜ë³„ ëŒ€í™” ì €ì¥
SESSIONS: Dict[str, List[BaseMessage]] = {}
SESSION_SUMMARY: Dict[str, str] = {}

# ----- í† í° ê³„ì‚° í•¨ìˆ˜ -----
def num_tokens_from_messages(messages: List[BaseMessage], model: str = MODEL_NAME) -> int:
    enc = tiktoken.encoding_for_model(model) if model in tiktoken.list_encoding_names() else tiktoken.get_encoding("cl100k_base")
    total = 0
    for m in messages:
        total += len(enc.encode(m.content))
    return total

def num_tokens_from_text(text: str, model: str = MODEL_NAME) -> int:
    enc = tiktoken.encoding_for_model(model) if model in tiktoken.list_encoding_names() else tiktoken.get_encoding("cl100k_base")
    return len(enc.encode(text))

# ----- ì˜¤ë˜ëœ ëŒ€í™” ìš”ì•½ í•¨ìˆ˜ -----
async def summarize_text(text: str) -> str:
    """ì˜¤ë˜ëœ íˆìŠ¤í† ë¦¬ë¥¼ ì§§ê²Œ ìš”ì•½"""
    if not text.strip():
        return ""
    if not API_KEY:
        # ğŸ’¡ API_KEYê°€ ì—†ì„ ê²½ìš° Mock ì‘ë‹µ
        return "[ìš”ì•½(Mock)] ì˜¤ë˜ëœ ëŒ€í™”ë¥¼ ìš”ì•½í–ˆìŠµë‹ˆë‹¤."
    llm = ChatOpenAI(model_name=MODEL_NAME, temperature=0, openai_api_key=API_KEY)
    prompt = f"ë‹¤ìŒ ëŒ€í™”ë¥¼ í•œ ë¬¸ë‹¨ìœ¼ë¡œ ë§¤ìš° ê°„ê²°íˆ ìš”ì•½í•´ì¤˜(í•œêµ­ì–´):\n\n{text}"
    result = await llm.apredict(prompt)
    return result.strip()

async def trim_context(session_id: str) -> None:
    """ëŒ€í™” í† í°ì´ ë§ì•„ì§€ë©´ ì˜¤ë˜ëœ ë‚´ìš© ìš”ì•½ í›„ ì œê±°"""
    history = SESSIONS.get(session_id, [])
    if not history:
        return

    summary = SESSION_SUMMARY.get(session_id, "")
    base: List[BaseMessage] = [SystemMessage(content=SYSTEM_PROMPT)]
    if summary:
        base.append(SystemMessage(content=f"[ì´ì „ ìš”ì•½]\n{summary}"))

    recent: List[BaseMessage] = []
    for msg in reversed(history):
        recent.append(msg)
        if num_tokens_from_messages(base + list(reversed(recent))) > MAX_CONTEXT_TOKENS:
            recent.pop()
            break

    kept = list(reversed(recent))
    removed_count = len(history) - len(kept)

    if removed_count > 0:
        old_text = "\n".join([m.content for m in history[:removed_count]])
        old_summary = await summarize_text(old_text)
        merged = (SESSION_SUMMARY.get(session_id, "") + "\n" + old_summary).strip()
        if num_tokens_from_text(merged) > 600:
            merged = await summarize_text(merged)
        SESSION_SUMMARY[session_id] = merged
        SESSIONS[session_id] = kept

# ----- ìŠ¤íŠ¸ë¦¬ë° ì½œë°± -----
class QueueCallback(AsyncCallbackHandler):
    def __init__(self, queue: asyncio.Queue):
        self.queue = queue

    async def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        """OpenAI APIê°€ í† í°ì„ ìƒì„±í•  ë•Œë§ˆë‹¤ í˜¸ì¶œë¨"""
        await self.queue.put(token)

    async def on_llm_end(self, *args, **kwargs) -> None:
        """ëª¨ë“  í† í° ì „ì†¡ í›„ í˜¸ì¶œ"""
        await self.queue.put("[[END]]")

# ----- ë¼ìš°íŒ… -----
@app.get("/", response_class=HTMLResponse)
async def index(request: Request):
    return templates.TemplateResponse("test.html", {"request": request})

@app.get("/stream")
async def stream(message: str, session_id: str = "") -> StreamingResponse:
    """í´ë¼ì´ì–¸íŠ¸ì—ì„œ /stream í˜¸ì¶œ ì‹œ, SSEë¡œ í† í° ì‹¤ì‹œê°„ ì „ì†¡"""
    if not session_id:
        session_id = str(uuid.uuid4())

    if session_id not in SESSIONS:
        SESSIONS[session_id] = []

    await trim_context(session_id)

    messages: List[BaseMessage] = [SystemMessage(content=SYSTEM_PROMPT)]
    summary = SESSION_SUMMARY.get(session_id, "")
    if summary:
        messages.append(SystemMessage(content=f"[ì´ì „ ìš”ì•½]\n{summary}"))
    messages.extend(SESSIONS[session_id])
    messages.append(HumanMessage(content=message))

    queue: asyncio.Queue = asyncio.Queue()

    async def token_generator() -> AsyncGenerator[bytes, None]:
        """SSEë¡œ í† í°ì„ í•˜ë‚˜ì”© ì „ì†¡"""
        if not API_KEY:
            # ğŸ’¡ API_KEY ì—†ì„ ë•Œ Mock í† í° ì „ì†¡
            mock_text = f"(Mock ì‘ë‹µ) '{message}'ì— ëŒ€í•œ ë‹µë³€ ì˜ˆì‹œì…ë‹ˆë‹¤."
            for ch in mock_text:
                await asyncio.sleep(0.05)  # íƒ€ì´í•‘ ì†ë„ í‰ë‚´
                yield f"data: {ch}\n\n".encode("utf-8")
            yield b"data: [[END]]\n\n"
            return

        # ì‹¤ì œ OpenAI API í˜¸ì¶œ
        cb = QueueCallback(queue)
        llm = ChatOpenAI(
            model_name=MODEL_NAME,
            temperature=TEMPERATURE,
            streaming=True,
            callbacks=[cb],
            openai_api_key=API_KEY
        )

        asyncio.create_task(llm.apredict_messages(messages))
        while True:
            token = await queue.get()
            if token == "[[END]]":
                yield b"data: [[END]]\n\n"
                break
            chunk = token.replace("\n", "\\n")
            yield f"data: {chunk}\n\n".encode("utf-8")

    SESSIONS[session_id].append(HumanMessage(content=message))

    headers = {
        "Cache-Control": "no-cache",
        "Content-Type": "text/event-stream",
        "X-Accel-Buffering": "no",
    }
    return StreamingResponse(token_generator(), headers=headers)




# uvicorn nenwsss:app --reload
#  http://127.0.0.1:8000


# fastapi
# uvicorn
# jinja2
# python-multipart
# langchain
# langchain-openai
# tiktoken
